{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import (\n",
    "    ModelConfig,\n",
    "    build_openclip_wrapper,\n",
    "    build_siglip2_wrapper,\n",
    "    build_vlm2vecv2_wrapper,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# OpenCLIP, native mode\n",
    "cfg_clip = ModelConfig(name=\"openclip\", device=device, mode=\"native\", precision=\"fp16\")\n",
    "openclip_model = build_openclip_wrapper(cfg_clip, \"ViT-B-16\", \"laion2b_s34b_b88k\")\n",
    "\n",
    "# SigLIP-2, native mode\n",
    "cfg_siglip = ModelConfig(name=\"siglip2\", device=device, mode=\"native\", precision=\"fp16\")\n",
    "siglip_model = build_siglip2_wrapper(cfg_siglip, \"google/siglip-2-base-patch16-224\")\n",
    "\n",
    "# VLM2Vec-V2, shared mode (for ablation)\n",
    "cfg_vlm = ModelConfig(name=\"vlm2vecv2\", device=device, mode=\"shared\", precision=\"fp16\")\n",
    "vlm_model = build_vlm2vecv2_wrapper(cfg_vlm, \"tiger-research/vlm2vec-v2-base\")\n",
    "\n",
    "# Example call:\n",
    "img_embeds, img_stats = openclip_model.encode_images(list_of_pil_images, return_stats=True)\n",
    "txt_embeds, txt_stats = openclip_model.encode_texts(list_of_captions, return_stats=True)\n",
    "print(img_stats, txt_stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
